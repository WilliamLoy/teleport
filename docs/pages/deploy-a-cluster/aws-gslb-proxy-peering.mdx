---
title: "AWS Route 53 GSLB Multi-Region Proxy Peering High Availability Deployment Guide"
description: "Deploying a Proxy-peered High Availability Teleport Cluster using Route 53 to create Global Server Load Balancing"
---

When deploying Teleport in production, you should design your deployment to
ensure that users can continue to access infrastructure during an incident in
your Teleport cluster. You should also make it possible to scale your Auth
Service and Proxy Service as you register more users and resources with
Teleport. 

(!docs/pages/includes/cloud/call-to-action.mdx!)

## Overview
This deployment architecure makes all connected resources accessible through a single Teleport cluster 
across multiple regions using exclusively AWS ecosystem infrastructure. 

This is accomplished using AWS Route 53 to create Global Server Load Balancing (GSLB) 
for the Teleport Cluster and Teleport Proxy Peering to reduce the number of connections 
created through the cluster.

This deployment architecture isn’t recommended for use cases where your users or resources are 
clustered in a single region or for Managed Service Providers needing to provide separate clusters 
to customers. Additionally, this architecture is not a solution for increasing the scalability of 
a single cluster.

We recommend this for globally distributed resources and employees that need a single point of 
access and need reduced latency when accessing resources. 

### Key deployment components
- High Availability Teleport Cluster
- Auth Servers must remain in a single region
- Proxies are deployed across multiple regions
- [AWS Route 53 latency based routing]([Latency-based routing](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html) 
       to create [GSLB](https://www.cloudflare.com/learning/cdn/glossary/global-server-load-balancing-gslb/)
- [Teleport TLS Routing](https://goteleport.com/docs/architecture/tls-routing/) to reduce the number of ports needed to use Teleport
- [Teleport Proxy Peering](https://goteleport.com/docs/architecture/proxy-peering/) for reducing the number of resource connections
- [AWS Network Load Balancing](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html)
- [AWS DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html) for cluster state storage
- [AWS S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) for session recording storage

## Advantages of this deployment architecture
- Eliminates the complexity of using and maintaining multiple clusters
- Uses the lowest latency path to connect users to resources
- Provides a highly-resilient, redundant HA architecture for Teleport that can quickly 
  scale with an organization’s needs.
- All required Teleport components can be provisioned within the AWS ecosystem.
- Using load balancers for the Proxy and Auth services allows for increased resilience during 
  Teleport upgrades as instances can be removed and added to the backend set within software 
  compatibility without impacting the user experience.

## Disadvantages of this deployment architecture
- Teleport Auth servers located in a single region can cause an outage of the cluster if 
  there is a failure of that region
- Technically complex to deploy
- Long-term cost may be a prohibitive factor for some organizations and can increase total 
  cost of ownership (TCO) throughout the system’s lifetime cycle.


![Diagram of a high-availability Teleport
architecture](../../img/deploy-a-cluster/teleport-ha-architecture.png)


## AWS Network load balancer(NLB)
For this deployment architecture we recommend using AWS NLBs if you plan 
to use Teleport TLS routing and Proxy Peering. The NLB forwards traffic 
from users and services to an available Teleport instance. This must not 
terminate TLS, and must transparently forward the TCP traffic it receives. 
In other words, this must be a Layer 4 load balancer, not a Layer 7 
(e.g., HTTP) load balancer. 

### Configure the NLBs
Configure the load balancer to forward traffic from the following ports on the
load balancer to the corresponding port on an available Teleport instance. The
configuration depends on whether you route Proxy Peering GRPC traffic over 
the public internet:

<Tabs>
<TabItem label="Public Internet">

| Port | Description |
| - | - |
| `443` | ALPN port for TLS Routing,  HTTPS connections to authenticate `tsh` users into the cluster, and to serve a Web UI |
| `3021`| Proxy Peering GRPC Stream  |

</TabItem>
<TabItem label="VPC peering">

These ports are required:

| Port | Description |
| - | - |
| `443` | ALPN port for TLS Routing,  HTTPS connections to authenticate `tsh` users into the cluster, and to serve a Web UI  |

</TabItem>
</Tabs>

We recommend enabling cross-zone load balancing in your NLB configuration to route traffic across multiple zones.

## Cluster state backend

The Teleport Auth Service stores cluster state (such as dynamic configuration
resources) and audit events as key/value pairs. In high-availability
deployments, you must configure the Auth Service to manage this data in a
key-value store that runs outside of your cluster of Teleport instances.

For Amazon DynamoDB, your Teleport configuration (which
we will describe in more detail in the [Configuration](#configuration) section)
names a table or collection where Teleport stores cluster state and audit
events. 

The Teleport Auth Service manages the creation of any required DynamoDB tables itself, 
and does not require them to exist in advance.

<Admonition title="Required permissions">

The Auth Service instances needs permissions to read from and write to DynamoDB, as well as 
to create tables.

</Admonition>

## Session recording backend

High-availability Teleport deployments use an object storage service for
persisting session recordings.

In your Teleport configuration (described in the [Configuration](#configuration)
section), you must name an S3 bucket to use for managing session recordings. The Teleport Auth
Service creates this bucket, so to prevent unexpected behavior, you should not
create it in advance. 

<Admonition title="Required permissions">

The Auth Service instances need permissions to get S3 buckets as well as to create, get, list, 
and update objects. Since this setup lets Teleport create buckets for you, you should also assign
Auth Service instances permissions to create buckets

</Admonition>

## TLS credential provisioning 

High-availability Teleport deployments require a  system to fetch TLS
credentials from a certificate authority like Let's Encrypt, AWS Certificate
Manager, Digicert, or a trusted internal authority. The system must then
provision Teleport Proxy Service instances with these credentials and renew them
periodically. 

If you are running a single instance of the Teleport Auth Service and Proxy
Service, you can configure this instance to fetch credentials for itself from
Let's Encrypt using the [ACME ALPN-01
challenge](https://letsencrypt.org/docs/challenge-types/#tls-alpn-01), where
Teleport demonstrates that it controls the ALPN server at the HTTPS address of
your Teleport Proxy Service. Teleport also fetches a separate certificate for
each application you have registered with Teleport, e.g.,
`grafana.teleport.example.com`. 

For high-availability deployments that use Let's Encrypt to supply TLS
credentials to Teleport instances running behind a load balancer, you will need
to use the [ACME
DNS-01](https://letsencrypt.org/docs/challenge-types/#dns-01-challenge)
challenge to demonstrate domain name ownership to Let's Encrypt. In this
challenge, your TLS credential provisioning system creates a DNS TXT record with
a value expected by Let's Encrypt.

In the configuration we are demonstrating in this guide, each Teleport Proxy
Service instance expects TLS credentials for HTTPS to be available at the file
paths `/etc/teleport-tls/tls.key` (private key) and `/etc/teleport-tls/tls.crt`
(certificate).

## Global Server Load Balancing with Route 53

[Latency-based routing](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html) 
in a private hosted zone must be used to ensure traffic from Teleport 
resources are routed to the closest or lowest latency path Proxy NLB based on the region of 
the VPC the resource is connecting from.

To create GSLB routing, create a CNAME record for each region you have VPCs containing Teleport connected resources.
It is recommeded to add a wildcard record for every region if you plan to use Teleport Appplication Access.

The following CNAME record values need to be set:
- **Value:** The domain name of the NLB where example-region-1 located Teleport resource traffic should be routed
- **Routing policy:** Latency
- **Region:** The AWS region from which traffic should be routed to the NLB listed in **Value**
- **Health Check ID:** It is recommended that you set this so that traffic is always routed to a healthy NLB

Example Hosted Zone using AWS Route53 Latency Routing to create GSLB:

### Root GSLB record for Teleport:

|Record name|Type|Value|
|---|---|---|
|```*teleport.example.com```|CNAME|AWS Route 53 nameservers|

### Teleport Proxy DNS records:
|Record name|Type|Routing Policy|Region|Value|
|---|---|---|---|---|
|```proxy.teleport.example.com```|CNAME|Latency|us-west-1| ```elb.us-west-1.amazonaws.com``` |
|```*.proxy.teleport.example.com```|CNAME|Latency|us-west-1| ```elb.us-west-1.amazonaws.com``` |
|```proxy.teleport.example.com```|CNAME|Latency|eu-central-1| ```elb.eu_central-1.amazonaws.com```|
|```*.proxy.teleport.example.com```|CNAME|Latency|eu-central-1| ```elb.eu_central-1.amazonaws.com```|

<Admonition title="Required permissions">

If you are using Let's Encrypt to provide TLS credentials to your Teleport
instances, the TLS credential system we mentioned earlier needs permissions to
manage Route53 DNS records in order to satisfy Let's Encrypt's DNS-01 challenge. 

</Admonition>

## Teleport instances

Run the Teleport Auth Service and Proxy Service as a scalable group of compute
resources, for example, a Kubernetes `Deployment` or AWS Auto Scaling group.
This requires running the `teleport` binary on each Kubernetes pod or virtual
machine or in your group. 

You should deploy your Teleport instances across multiple zones (if using a
cloud provider) or data centers (if using an on-premise solution) to ensure
availability.

In the [Configuration](#configuration) section, we will show you how to
configure each binary for high availability.

### Open ports

Ensure that, on each Teleport instance, the following ports allow traffic from
the load balancer. The Proxy Service uses these ports to communicate with
Teleport users and services.


| Port | Description |
| - | - |
| `443` | ALPN port for TLS Routing. |



### License file

If you are deploying Teleport Enterprise, you need to download a license file
and make it available to your Teleport Auth Service instances.

To obtain your license file, visit the [Teleport customer
dashboard](https://dashboard.gravitational.com/web/login) and log in. Click
"DOWNLOAD LICENSE KEY". You will see your current Teleport Enterprise account
permissions and the option to download your license file:

![License File modal](../../img/enterprise/license.png)

The license file must be available to each Teleport Auth Service instance at
`/var/lib/teleport/license.pem`. 

### Configuration

Create a configuration file and provide it to each of your Teleport instances at
`/etc/teleport.yaml`. We will explain the required configuration fields for a
high-availability Teleport deployment below. These are the minimum requirements,
and when planning your high-availability deployment, you will want to follow a
more specific [deployment guide](introduction.mdx) for your environment. 

#### `storage` 

The first configuration section to write is the `storage` section, which
configures the cluster state backend and session recording backend for the
Teleport Auth Service:

```yaml
version: v3
teleport:
  storage:
    # ...
```

Consult our [Backends Reference](../reference/backends.mdx) for the configuration
fields you should set in the `storage` section.

#### `auth_service` and `proxy_service` 

The `auth_service` and `proxy_service` sections configure the Auth Service and
Proxy Service, which we will run together on each Teleport instance. The
configuration will depend on whether you are enabling TLS Routing in your
cluster:

<Tabs>
<TabItem label="TLS Routing">

To enable TLS Routing in your Teleport cluster, add the following to your
Teleport configuration:

```yaml
version: v3
teleport:
  storage:
  # ...
auth_service:
  enabled: true
  cluster_name: "mycluster.example.com"
  # Remove this if not using Teleport Enterprise
  license_file: "/var/lib/license/license.pem"
proxy_service:
  enabled: true
  public_addr: "mycluster.example.com:443"
  https_keypairs:
  - key_file: /etc/teleport-tls/tls.key
    cert_file: /etc/teleport-tls/tls.crt
```

This configuration has no fields specific to TLS Routing. In `v2`, the
configuration version we are using here, TLS Routing is enabled by default.

</TabItem>
<TabItem label="Separate Listeners">

To disable TLS Routing in your Teleport cluster, add the following to your
Teleport configuration:

```yaml
version: v3
teleport:
  storage:
  # ...
auth_service:
  proxy_listener_mode: separate
  enabled: true
  cluster_name: "mycluster.example.com"
  # Remove this if not using Teleport Enterprise
  license_file: "/var/lib/license/license.pem"
proxy_service:
  enabled: true
  listen_addr: 0.0.0.0:3023
  tunnel_listen_addr: 0.0.0.0:3024
  public_addr: "mycluster.example.com:443"
  https_keypairs:
  - key_file: /etc/teleport-tls/tls.key
    cert_file: /etc/teleport-tls/tls.crt
```

This configuration assigns `auth_service.proxy_listener_mode` to `separate` to
disable TLS Routing. It also explicitly assigns an SSH port (`listen_addr`) and
reverse tunnel port (`tunnel_listen_addr`) for the Proxy Service.

</TabItem>
</Tabs>

The `auth_service` and `proxy_service` configurations above have the following
required settings for a high-availability Teleport deployment:

- In the `auth_service` section, we have enabled the Teleport Auth Service
  (`enabled`) and instructed it to find an Enterprise license file at
  `/var/lib/license/license.pem` (`license_file`). Remove the `license_file`
  field if you are deploying the open source edition of Teleport. 
- In the `proxy_service` section, we have enabled the Teleport Proxy Service
  (`enabled`) and instructed it to find its TLS credentials in the
  `/etc/teleport-tls` directory (`https_keypairs`).

#### `ssh_service`

You can disable the SSH Service on each Teleport instance by adding the
following to each instance's configuration file: 

```yaml
version: v3
teleport:
  storage:
  # ...
auth_service:
# ...
proxy_service:
# ...
ssh_service:
  enabled: false
```

This is suitable for deploying Teleport on Kubernetes, where the `teleport` pod
should not have direct access to the underlying node. 

If you are deploying Teleport on a cluster of virtual machines, remove this line
to run the SSH Service and enable secure access to the host.

## Next steps

### Refine your plan

Now that you know the general principles behind a high-availability Teleport
deployment, read about how to design your own deployment on Kubernetes or a
cluster of virtual machines in your cloud of choice:

- [High-availability Teleport Deployments on Kubernetes with
  Helm](helm-deployments.mdx)
- [Reference Deployments](deployments.mdx) for running Teleport on a cluster of
  virtual machines

### Ensure high performance

You should also get familiar with how to ensure that your Teleport deployment is
performing as expected:

- [Scaling a Teleport cluster](../management/operations/scaling.mdx)
- [Monitoring a Teleport cluster](../management/diagnostics.mdx)

### Deploy Teleport services

Once your high-availability Teleport deployment is up and running, you can add
resources by launching Teleport services. You can run these services in a
separate network from your Teleport cluster. 

To get started, read about registering:

- [Applications](../application-access/getting-started.mdx)
- [Servers](../server-access/getting-started.mdx)
- [Kubernetes clusters](../kubernetes-access/getting-started.mdx)
- [Databases](../database-access/getting-started.mdx)
- [Windows desktops](../desktop-access/getting-started.mdx)
- [Bot users](../machine-id/getting-started.mdx)
