---
title: "AWS Route 53 GSLB Multi-Region Proxy Peering High Availability Deployment Guide"
description: "Deploying a Proxy-peered High Availability Teleport Cluster using Route 53 to create Global Server Load Balancing"
---

When deploying Teleport in production, you should design your deployment to
ensure that users can continue to access infrastructure during an incident in
your Teleport cluster. You should also make it possible to scale your Auth
Service and Proxy Service as you register more users and resources with
Teleport. 

(!docs/pages/includes/cloud/call-to-action.mdx!)

## Overview
In deployment architecture, the Teleport cluster is deployed as a single cluster across multiple regions
using exclusively AWS ecosystem infrastructure. 
This is a complex deployment architecture. This deployment architecture deploys Auth servers in 
a single region and Proxy servers in many regions. This is accomplished using AWS Route 53 
to create Global Server Load Balancing (GSLB) for the Teleport Cluster and Teleport Proxy 
Peering to reduce the number of connections created through the cluster.

This deployment architecture isn’t recommended for use cases where your users or resources are 
clustered in a single region or for Managed Service Providers needing to provide separate clusters 
to customers. Additionally, this architecture is not a solution for increasing the scalability of 
a single cluster.

We recommend this for globally distributed resources and employees that need a single point of 
access and need reduced latency when accessing resources. 

### Key deployment components
- High Availability Teleport Cluster
- [AWS Route 53 latency based routing]([Latency-based routing](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html) 
       to create [GSLB](https://www.cloudflare.com/learning/cdn/glossary/global-server-load-balancing-gslb/)
- [Teleport TLS Routing](https://goteleport.com/docs/architecture/tls-routing/) to reduce the number of ports needed to use Teleport
- [Teleport Proxy Peering](https://goteleport.com/docs/architecture/proxy-peering/) for reducing the number of resource connections
- [AWS Network Load Balancing](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html)
- [AWS DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html) for cluster state storage
- [AWS S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) for session recording storage

## Advantages of this deployment architecture
- Eliminates the complexity of using and maintaining multiple clusters
- Uses the lowest latency path to connect users to resources
- Provides a highly-resilient, redundant HA architecture for Teleport that can quickly 
  scale with an organization’s needs.
- All required Teleport components can be provisioned within the AWS ecosystem.
- Using load balancers for the Proxy and Auth services allows for increased resilience during 
  Teleport upgrades as instances can be removed and added to the backend set within software 
  compatibility without impacting the user experience.

## Disadvantages of this deployment architecture
- Teleport Auth servers located in a single region can cause an outage of the cluster if 
  there is a failure of that region
- Technically complex to deploy
- Long-term cost may be a prohibitive factor for some organizations and can increase total 
  cost of ownership (TCO) throughout the system’s lifetime cycle.


![Diagram of a high-availability Teleport
architecture](../../img/deploy-a-cluster/teleport-ha-architecture.png)


## AWS Network load balancer(NLB)
For this deployment architecture we recommend using AWS NLBs if you plan 
to use Teleport TLS routing and Proxy Peering. The NLB forwards traffic 
from users and services to an available Teleport instance. This must not 
terminate TLS, and must transparently forward the TCP traffic it receives. 
In other words, this must be a Layer 4 load balancer, not a Layer 7 
(e.g., HTTP) load balancer. 

### Configure the NLB
Configure the load balancer to forward traffic from the following ports on the
load balancer to the corresponding port on an available Teleport instance. The
configuration depends on whether you route Proxy Peering GRPC traffic over 
the public internet:

<Tabs>
<TabItem label="Public Internet">

| Port | Description |
| - | - |
| `443` | ALPN port for TLS Routing,  HTTPS connections to authenticate `tsh` users into the cluster, and to serve a Web UI |
| `3021`| Proxy Peering GRPC Stream  |

</TabItem>
<TabItem label="VPC peering">

These ports are required:

| Port | Description |
| - | - |
| `443` | ALPN port for TLS Routing,  HTTPS connections to authenticate `tsh` users into the cluster, and to serve a Web UI  |

</TabItem>
</Tabs>

We recommend enabling cross-zone load balancing in your NLB configuration to route traffic across multiple zones.

### TLS Routing

Your load balancer configuration depends on whether you will enable [TLS
Routing](../management/operations/tls-routing.mdx) in your Teleport cluster.

With TLS Routing, the Teleport Proxy Service uses application-layer protocol
negotiation (ALPN) to handle all communication with users and services via the
HTTPS port, regardless of protocol. Without TLS Routing, the Proxy Service
listens on separate ports for each protocol.

The advantage of TLS Routing is its simplicity: you can expose only a single
port on the load balancer to the public internet.

The disadvantage of TLS Routing is that it is impossible to implement Layer 7
load balancing for HTTPS traffic, since traffic that reaches the HTTPS port can
use any supported protocol. 

The approach we describe in this guide uses only a Layer 4 load balancer to
minimize the infrastructure you will deploy, but users that require a separate
load balancer for HTTPS traffic should disable TLS Routing.

## Cluster state backend

The Teleport Auth Service stores cluster state (such as dynamic configuration
resources) and audit events as key/value pairs. In high-availability
deployments, you must configure the Auth Service to manage this data in a
key-value store that runs outside of your cluster of Teleport instances.

The Auth Service supports the following backends for cluster state and audit
events:

- Amazon DynamoDB
- Google Cloud Firestore
- etcd (cluster state only)

For Amazon DynamoDB and Google Cloud Firestore, your Teleport configuration (which
we will describe in more detail in the [Configuration](#configuration) section)
names a table or collection where Teleport stores cluster state and audit
events. 

The Teleport Auth Service manages the creation of any required DynamoDB tables
or Firestore collections itself, and does not require them to exist in advance.

The Auth Service can also store cluster state in self-hosted
[etcd](https://etcd.io/) deployments. In this case, Teleport uses namespaces
within item keys to identify cluster state data.

<Admonition title="Required permissions">

In your cloud provider's RBAC solution (e.g., AWS or Google Cloud IAM), your
Auth Service instances need permissions to read from and write to your chosen
key/value store, as well as to create tables and collections (if your key/value
store supports them).

</Admonition>

## Session recording backend

High-availability Teleport deployments use an object storage service for
persisting session recordings. The Teleport Auth Service supports two object
storage services:

- Amazon S3 (or an S3-compatible object store)
- Google Cloud Storage

In your Teleport configuration (described in the [Configuration](#configuration)
section), you must name a bucket within Google Cloud Storage or an Amazon
S3-compatible service to use for managing session recordings. The Teleport Auth
Service creates this bucket, so to prevent unexpected behavior, you should not
create it in advance. 

<Admonition title="Required permissions">

In your cloud provider's RBAC solution, your Auth Service instances need
permissions to get buckets as well as to create, get, list, and update objects.
Since this setup lets Teleport create buckets for you, you should also assign
Auth Service instances permissions to create buckets

In Google Cloud Storage, Auth Service instances also need permissions to delete
objects. 

</Admonition>

## TLS credential provisioning 

High-availability Teleport deployments require a  system to fetch TLS
credentials from a certificate authority like Let's Encrypt, AWS Certificate
Manager, Digicert, or a trusted internal authority. The system must then
provision Teleport Proxy Service instances with these credentials and renew them
periodically. 

If you are running a single instance of the Teleport Auth Service and Proxy
Service, you can configure this instance to fetch credentials for itself from
Let's Encrypt using the [ACME ALPN-01
challenge](https://letsencrypt.org/docs/challenge-types/#tls-alpn-01), where
Teleport demonstrates that it controls the ALPN server at the HTTPS address of
your Teleport Proxy Service. Teleport also fetches a separate certificate for
each application you have registered with Teleport, e.g.,
`grafana.teleport.example.com`. 

For high-availability deployments that use Let's Encrypt to supply TLS
credentials to Teleport instances running behind a load balancer, you will need
to use the [ACME
DNS-01](https://letsencrypt.org/docs/challenge-types/#dns-01-challenge)
challenge to demonstrate domain name ownership to Let's Encrypt. In this
challenge, your TLS credential provisioning system creates a DNS TXT record with
a value expected by Let's Encrypt.

In the configuration we are demonstrating in this guide, each Teleport Proxy
Service instance expects TLS credentials for HTTPS to be available at the file
paths `/etc/teleport-tls/tls.key` (private key) and `/etc/teleport-tls/tls.crt`
(certificate).

## Global Server Load Balancing with Route 53

[Latency-based routing](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html) 
in a private hosted zone must be used to ensure traffic from Teleport 
resources are routed to the closest or lowest latency path Proxy NLB based on the region of 
the VPC the resource is connecting from.

To create GSLB routing, create a CNAME record for each region you have VPCs containing Teleport connected resources.

The following CNAME record values need to be set:
- Value: The domain name of the NLB where example-region-1 located Teleport resource traffic should be routed
- Routing policy: Latency
- Location: The region from which traffic should be routed to the NLB listed in Value
- Health Check ID: It is recommended that you set this so that traffic is always routed to a healthy NLB

Teleport resource agents must be configured to point ```proxy_server:``` to the GSLB domain configured in Route 53

### Teleport Proxy Service records

Users and services must be able to reach the Teleport Proxy Service in order to
connect to your Teleport cluster. Since a high availability setup runs Teleport
instances behind a load balancer, you must create a DNS record that points to
the load balancer. 

Depending on how your infrastructure's DNS is organized, this will be one of the
following, assuming your domain is `example.com`:

|Record Type|Domain Name|Value|
|---|---|---|
|A|teleport.example.com|The IP address of your load balancer|
|CNAME|teleport.example.com|The domain name of your load balancer|

### Registering applications with Teleport 

Teleport assigns a subdomain to each application you have connected to Teleport
(e.g., `grafana.teleport.example.com`), so you will need to ensure that a DNS
record exists for each application-specific subdomain so clients can access your
applications via Teleport. 

You should create either a separate DNS record for each subdomain or a single
record with a wildcard subdomain such as `*.teleport.example.com`. 

Create one of the following wildcard DNS records so you can register any
application with Teleport:

|Record Type|Domain Name|Value|
|---|---|---|
|A|*.teleport.example.com|The IP address of your load balancer|
|CNAME|*.teleport.example.com|The domain name of your load balancer|

<Admonition title="Required permissions">

If you are using Let's Encrypt to provide TLS credentials to your Teleport
instances, the TLS credential system we mentioned earlier needs permissions to
manage DNS records in order to satisfy Let's Encrypt's DNS-01 challenge. 

If you are using cloud-managed solutions, you should use your cloud provider's
RBAC system (e.g., AWS IAM) to grant a role to the Proxy Service to manage DNS
records. 

</Admonition>

## Teleport instances

Run the Teleport Auth Service and Proxy Service as a scalable group of compute
resources, for example, a Kubernetes `Deployment` or AWS Auto Scaling group.
This requires running the `teleport` binary on each Kubernetes pod or virtual
machine or in your group. 

You should deploy your Teleport instances across multiple zones (if using a
cloud provider) or data centers (if using an on-premise solution) to ensure
availability.

In the [Configuration](#configuration) section, we will show you how to
configure each binary for high availability.

### Open ports

Ensure that, on each Teleport instance, the following ports allow traffic from
the load balancer. The Proxy Service uses these ports to communicate with
Teleport users and services.

As with your load balancer configuration, the ports you should open on your
Teleport instances depend on whether you will enable TLS Routing: 

<Tabs>
<TabItem label="TLS Routing">

| Port | Description |
| - | - |
| `443` | ALPN port for TLS Routing. |

</TabItem>
<TabItem label="Separate Ports">

These ports are required:

| Port | Description |
| - | - |
| `3023` | SSH port for clients connect to. |
| `3024` | SSH port used to create reverse SSH tunnels from behind-firewall environments. |
| `443` | HTTPS connections to authenticate `tsh` users into the cluster. The same connection is used to serve a Web UI. |

You can leave these ports closed if you are not using their corresponding
services:

| Port | Description |
| - | - |
| `3026` | HTTPS Kubernetes proxy |
| `3036` | MySQL port |
| `5432` | Postgres port |

</TabItem>
</Tabs>

*This is the same table of ports you used to configure the load balancer.*

### License file

If you are deploying Teleport Enterprise, you need to download a license file
and make it available to your Teleport Auth Service instances.

To obtain your license file, visit the [Teleport customer
dashboard](https://dashboard.gravitational.com/web/login) and log in. Click
"DOWNLOAD LICENSE KEY". You will see your current Teleport Enterprise account
permissions and the option to download your license file:

![License File modal](../../img/enterprise/license.png)

The license file must be available to each Teleport Auth Service instance at
`/var/lib/teleport/license.pem`. 

### Configuration

Create a configuration file and provide it to each of your Teleport instances at
`/etc/teleport.yaml`. We will explain the required configuration fields for a
high-availability Teleport deployment below. These are the minimum requirements,
and when planning your high-availability deployment, you will want to follow a
more specific [deployment guide](introduction.mdx) for your environment. 

#### `storage` 

The first configuration section to write is the `storage` section, which
configures the cluster state backend and session recording backend for the
Teleport Auth Service:

```yaml
version: v3
teleport:
  storage:
    # ...
```

Consult our [Backends Reference](../reference/backends.mdx) for the configuration
fields you should set in the `storage` section.

#### `auth_service` and `proxy_service` 

The `auth_service` and `proxy_service` sections configure the Auth Service and
Proxy Service, which we will run together on each Teleport instance. The
configuration will depend on whether you are enabling TLS Routing in your
cluster:

<Tabs>
<TabItem label="TLS Routing">

To enable TLS Routing in your Teleport cluster, add the following to your
Teleport configuration:

```yaml
version: v3
teleport:
  storage:
  # ...
auth_service:
  enabled: true
  cluster_name: "mycluster.example.com"
  # Remove this if not using Teleport Enterprise
  license_file: "/var/lib/license/license.pem"
proxy_service:
  enabled: true
  public_addr: "mycluster.example.com:443"
  https_keypairs:
  - key_file: /etc/teleport-tls/tls.key
    cert_file: /etc/teleport-tls/tls.crt
```

This configuration has no fields specific to TLS Routing. In `v2`, the
configuration version we are using here, TLS Routing is enabled by default.

</TabItem>
<TabItem label="Separate Listeners">

To disable TLS Routing in your Teleport cluster, add the following to your
Teleport configuration:

```yaml
version: v3
teleport:
  storage:
  # ...
auth_service:
  proxy_listener_mode: separate
  enabled: true
  cluster_name: "mycluster.example.com"
  # Remove this if not using Teleport Enterprise
  license_file: "/var/lib/license/license.pem"
proxy_service:
  enabled: true
  listen_addr: 0.0.0.0:3023
  tunnel_listen_addr: 0.0.0.0:3024
  public_addr: "mycluster.example.com:443"
  https_keypairs:
  - key_file: /etc/teleport-tls/tls.key
    cert_file: /etc/teleport-tls/tls.crt
```

This configuration assigns `auth_service.proxy_listener_mode` to `separate` to
disable TLS Routing. It also explicitly assigns an SSH port (`listen_addr`) and
reverse tunnel port (`tunnel_listen_addr`) for the Proxy Service.

</TabItem>
</Tabs>

The `auth_service` and `proxy_service` configurations above have the following
required settings for a high-availability Teleport deployment:

- In the `auth_service` section, we have enabled the Teleport Auth Service
  (`enabled`) and instructed it to find an Enterprise license file at
  `/var/lib/license/license.pem` (`license_file`). Remove the `license_file`
  field if you are deploying the open source edition of Teleport. 
- In the `proxy_service` section, we have enabled the Teleport Proxy Service
  (`enabled`) and instructed it to find its TLS credentials in the
  `/etc/teleport-tls` directory (`https_keypairs`).

#### `ssh_service`

You can disable the SSH Service on each Teleport instance by adding the
following to each instance's configuration file: 

```yaml
version: v3
teleport:
  storage:
  # ...
auth_service:
# ...
proxy_service:
# ...
ssh_service:
  enabled: false
```

This is suitable for deploying Teleport on Kubernetes, where the `teleport` pod
should not have direct access to the underlying node. 

If you are deploying Teleport on a cluster of virtual machines, remove this line
to run the SSH Service and enable secure access to the host.

## Next steps

### Refine your plan

Now that you know the general principles behind a high-availability Teleport
deployment, read about how to design your own deployment on Kubernetes or a
cluster of virtual machines in your cloud of choice:

- [High-availability Teleport Deployments on Kubernetes with
  Helm](helm-deployments.mdx)
- [Reference Deployments](deployments.mdx) for running Teleport on a cluster of
  virtual machines

### Ensure high performance

You should also get familiar with how to ensure that your Teleport deployment is
performing as expected:

- [Scaling a Teleport cluster](../management/operations/scaling.mdx)
- [Monitoring a Teleport cluster](../management/diagnostics.mdx)

### Deploy Teleport services

Once your high-availability Teleport deployment is up and running, you can add
resources by launching Teleport services. You can run these services in a
separate network from your Teleport cluster. 

To get started, read about registering:

- [Applications](../application-access/getting-started.mdx)
- [Servers](../server-access/getting-started.mdx)
- [Kubernetes clusters](../kubernetes-access/getting-started.mdx)
- [Databases](../database-access/getting-started.mdx)
- [Windows desktops](../desktop-access/getting-started.mdx)
- [Bot users](../machine-id/getting-started.mdx)
